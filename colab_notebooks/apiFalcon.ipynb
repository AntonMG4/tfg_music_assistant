{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqXXkbQcxr3a"
   },
   "source": [
    "# API para usar las funciones de predicci\u00f3n de Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77709,
     "status": "ok",
     "timestamp": 1751384577003,
     "user": {
      "displayName": "Ant\u00f3n Maestre G\u00f3mez",
      "userId": "08293447014606852871"
     },
     "user_tz": -120
    },
    "id": "VRSs7rM-cuoF",
    "outputId": "c51d5329-e456-4221-aa6c-108da362babf"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. Instalar dependencias\n",
    "# ================================\n",
    "!pip install transformers accelerate\n",
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 15648,
     "status": "ok",
     "timestamp": 1751384595127,
     "user": {
      "displayName": "Ant\u00f3n Maestre G\u00f3mez",
      "userId": "08293447014606852871"
     },
     "user_tz": -120
    },
    "id": "ZvE0kXUKc3bu"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 2. Importar librer\u00edas necesarias\n",
    "# ================================\n",
    "import threading\n",
    "import time\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "35a240f6e08b44d785c63ce13a71eb14",
      "0bda8a0b4b034ac49ba217687a119b98",
      "e04d511772384575ba8161451a4b96cd",
      "39550fc39fd24f5ea48c975314e2abdb",
      "cc85c92ec4904ca58bec584140935ce4",
      "a304890d6aba493f8a7a08a5a960c92e",
      "04668de17c444164a1aaf01e9b83c397",
      "01d8513deafb41f1a7b0710242228eaa",
      "e40dced479714361b3f0473ecacccbef",
      "ac4b07bb617b45ef9b1c24a129d04973",
      "1b1f61da0fd6418bbf3c72b5ff301d59"
     ]
    },
    "executionInfo": {
     "elapsed": 283439,
     "status": "ok",
     "timestamp": 1751384931897,
     "user": {
      "displayName": "Ant\u00f3n Maestre G\u00f3mez",
      "userId": "08293447014606852871"
     },
     "user_tz": -120
    },
    "id": "_9YSEcPeaUXJ",
    "outputId": "e9f35a25-1a40-4fdd-dac4-14a8bc77ce32"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 3. Cargar el modelo ya fusionado (sin LoRA)\n",
    "# ================================\n",
    "model_path = \"/content/drive/MyDrive/Colab Notebooks/ft_falcon_model/final_model/falcon_merged\"\n",
    "\n",
    "# Montar directorio de drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",               # permite usar parcialmente la GPU + CPU\n",
    "    torch_dtype=torch.float16,      # reduce el uso de memoria\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=3,\n",
    "        do_sample=False,\n",
    "        return_full_text=False,\n",
    "        truncation=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# ================================\n",
    "# 4. Crear servidor Flask\n",
    "# ================================\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "  data = request.get_json()\n",
    "  prompt = data.get(\"prompt\", \"\")\n",
    "  result = pipe(prompt)\n",
    "  if not result or 'generated_text' not in result[0]:\n",
    "    label = 'view'\n",
    "    return jsonify({\"intention\": label})\n",
    "\n",
    "\n",
    "  text = re.sub(r'[^a-z]', ' ', result[0]['generated_text'].strip().lower()).strip()\n",
    "  words = text.split()\n",
    "\n",
    "  if not words:  # Verificar si la lista est\u00e1 vac\u00eda\n",
    "    label = 'view'\n",
    "    return jsonify({\"intention\": label})\n",
    "  text = words[0]  # Tomar solo la primera palabra\n",
    "\n",
    "  # Clasificaci\u00f3n\n",
    "  if 'clear' in text:\n",
    "    label = 'clear'\n",
    "  elif 'view' in text:\n",
    "    label = 'view'\n",
    "  elif 'add' in text:\n",
    "    label = 'add'\n",
    "  elif 'remove' in text:\n",
    "    label = 'remove'\n",
    "  else:\n",
    "    label = 'view'\n",
    "  return jsonify({\"intention\": label})\n",
    "\n",
    "# ================================\n",
    "# 5. Ejecutar Flask y LocalTunnel\n",
    "# ================================\n",
    "def run_flask():\n",
    "    app.run(port=5001)\n",
    "\n",
    "def run_tunnel():\n",
    "    get_ipython().system_raw('lt --port 5001 > tunnel.txt 2>&1 &')\n",
    "    time.sleep(4)\n",
    "    !curl -s http://localhost:4040/api/tunnels | grep -o '\"public_url\":\"[^\"]*' | grep -o 'http[^\"]*'\n",
    "\n",
    "threading.Thread(target=run_flask).start()\n",
    "time.sleep(2)\n",
    "run_tunnel()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5119,
     "status": "ok",
     "timestamp": 1751384940042,
     "user": {
      "displayName": "Ant\u00f3n Maestre G\u00f3mez",
      "userId": "08293447014606852871"
     },
     "user_tz": -120
    },
    "id": "Leo-RtLGlnNk",
    "outputId": "58f97667-0c65-4a2e-c0de-345cb76ad6e7"
   },
   "outputs": [],
   "source": [
    "# Relanzar el t\u00fanel y mostrar la salida completa\n",
    "import time\n",
    "get_ipython().system_raw('lt --port 5001 > tunnel.txt 2>&1 &')\n",
    "time.sleep(5)\n",
    "\n",
    "# Ver toda la salida generada por localtunnel\n",
    "!cat tunnel.txt\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPfDzAd7kSp6fT3Azi7toPZ",
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}