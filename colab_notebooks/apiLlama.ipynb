{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqXXkbQcxr3a"
   },
   "source": [
    "# API para usar las funciones de predicción de LLaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122957,
     "status": "ok",
     "timestamp": 1750791925203,
     "user": {
      "displayName": "Antón Maestre Gómez",
      "userId": "08293447014606852871"
     },
     "user_tz": -120
    },
    "id": "VRSs7rM-cuoF",
    "outputId": "e38dca20-ded7-4dd3-fe21-f1294ef540bd"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. Instalar dependencias\n",
    "# ================================\n",
    "!pip install transformers accelerate\n",
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 19172,
     "status": "ok",
     "timestamp": 1750791970612,
     "user": {
      "displayName": "Antón Maestre Gómez",
      "userId": "08293447014606852871"
     },
     "user_tz": -120
    },
    "id": "ZvE0kXUKc3bu"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 2. Importar librerías necesarias\n",
    "# ================================\n",
    "import threading\n",
    "import time\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89910,
     "status": "ok",
     "timestamp": 1750792065324,
     "user": {
      "displayName": "Antón Maestre Gómez",
      "userId": "08293447014606852871"
     },
     "user_tz": -120
    },
    "id": "5x4x_6otc6hL",
    "outputId": "b5e2bff9-4518-4c31-9a01-9bb3d5fc1358"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 3. Cargar el modelo ya fusionado (sin LoRA)\n",
    "# ================================\n",
    "model_path = \"/content/drive/MyDrive/Colab Notebooks/ft_llama_model/final_model/llama_merged\"\n",
    "\n",
    "# Montar directorio de drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=3,\n",
    "        do_sample=False,\n",
    "        return_full_text=False,\n",
    "        truncation=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# ================================\n",
    "# 4. Crear servidor Flask\n",
    "# ================================\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "  data = request.get_json()\n",
    "  prompt = data.get(\"prompt\", \"\")\n",
    "  result = pipe(prompt)\n",
    "  if not result or 'generated_text' not in result[0]:\n",
    "    label = 'view'\n",
    "    return jsonify({\"intention\": label})\n",
    "\n",
    "  text = result[0]['generated_text'].strip().lower().replace(\"**\", \"\")\n",
    "\n",
    "  # Clasificación\n",
    "  if 'clear' in text:\n",
    "    label = 'clear'\n",
    "  elif 'view' in text:\n",
    "    label = 'view'\n",
    "  elif 'add' in text:\n",
    "    label = 'add'\n",
    "  elif 'remove' in text:\n",
    "    label = 'remove'\n",
    "  else:\n",
    "    label = 'view'\n",
    "  return jsonify({\"intention\": label})\n",
    "\n",
    "# ================================\n",
    "# 5. Ejecutar Flask y LocalTunnel\n",
    "# ================================\n",
    "def run_flask():\n",
    "    app.run(port=5000)\n",
    "\n",
    "def run_tunnel():\n",
    "    get_ipython().system_raw('lt --port 5000 > tunnel.txt 2>&1 &')\n",
    "    time.sleep(4)\n",
    "    !curl -s http://localhost:4040/api/tunnels | grep -o '\"public_url\":\"[^\"]*' | grep -o 'http[^\"]*'\n",
    "\n",
    "threading.Thread(target=run_flask).start()\n",
    "time.sleep(2)\n",
    "run_tunnel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5140,
     "status": "ok",
     "timestamp": 1750793039377,
     "user": {
      "displayName": "Antón Maestre Gómez",
      "userId": "08293447014606852871"
     },
     "user_tz": -120
    },
    "id": "Bwzj0a4vricZ",
    "outputId": "7d568a14-50f8-4a0d-bf08-1a0c4b8e7bb8"
   },
   "outputs": [],
   "source": [
    "# Relanzar el túnel y mostrar la salida completa\n",
    "import time\n",
    "get_ipython().system_raw('lt --port 5000 > tunnel.txt 2>&1 &')\n",
    "time.sleep(5)\n",
    "\n",
    "# Ver toda la salida generada por localtunnel\n",
    "!cat tunnel.txt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOhPywwWgRPPmzuVxW+Pq3R",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
