# ğŸµ Music Assistant based on LLMs & NLP

> **Trabajo de Fin de Grado (TFG) - Grado en IngenierÃ­a InformÃ¡tica**
> *Escuela TÃ©cnica Superior de IngenierÃ­a (ETSI), Universidad de Huelva*
>
> **Autor:** AntÃ³n Maestre GÃ³mez | **Tutor:** Jacinto Mata VÃ¡zquez

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white)](https://www.python.org/)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Transformers-yellow)](https://huggingface.co/)
[![Colab](https://img.shields.io/badge/Google-Colab-orange?logo=googlecolab)](https://colab.research.google.com/)
[![Framework](https://img.shields.io/badge/DialogueKit-Flask-green)](https://github.com/iai-group/DialogueKit)
[![Frontend](https://img.shields.io/badge/React-TypeScript-61DAFB?logo=react&logoColor=black)](https://react.dev/)
([https://img.shields.io/badge/PEFT-LoRA-red](https://img.shields.io/badge/PEFT-LoRA-red))]([https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index))
[![License](https://img.shields.io/badge/License-Apache%202.0-lightgrey.svg)]([https://opensource.org/licenses/Apache-2.0](https://opensource.org/licenses/Apache-2.0))

## ğŸ“„ DescripciÃ³n del Proyecto

Este proyecto presenta el desarrollo de un **asistente musical conversacional** capaz de interpretar peticiones en lenguaje natural y gestionar una lista de reproducciÃ³n en tiempo real.

A diferencia de los asistentes tradicionales basados en comandos rÃ­gidos, este sistema utiliza **Modelos de Lenguaje Generativos (LLMs)** ajustados mediante tÃ©cnicas de *Fine-Tuning* para clasificar la intenciÃ³n del usuario. El sistema actÃºa como un orquestador inteligente que traduce lenguaje natural en operaciones SQL sobre una base de datos musical.

### Funcionalidades Principales
El modelo clasifica cada interacciÃ³n del usuario en una de las cuatro intenciones soportadas:
*   âœ… **Add:** AÃ±adir una canciÃ³n especÃ­fica a la playlist (ej: *"Pon 'Bohemian Rhapsody' de Queen"*).
*   âŒ **Remove:** Eliminar una canciÃ³n concreta (ej: *"Quita esa canciÃ³n de la lista"*).
*   ğŸ‘€ **View:** Consultar el estado actual de la lista de reproducciÃ³n.
*   ğŸ—‘ï¸ **Clear:** Vaciar la lista completa de golpe.

---

## ğŸ§  Modelos y MetodologÃ­a

Para este trabajo se han comparado, optimizado y desplegado dos arquitecturas de *State-of-the-Art*:

| Modelo | ParÃ¡metros | Tipo | DescripciÃ³n |
| :--- | :--- | :--- | :--- |
| **LLaMA-3.2-1B-Instruct** | 1.2B | Meta | Modelo ligero optimizado para seguir instrucciones. |
| **Falcon-7B** | 7B | TII | Modelo entrenado en el corpus masivo RefinedWeb. |

### ğŸ”¬ Entrenamiento y OptimizaciÃ³n
*   **Dataset:** 1600 frases sintÃ©ticas generadas con ChatGPT, balanceadas perfectamente entre las 4 clases (400 ejemplos/clase).
*   **Fine-Tuning:** Se utilizÃ³ **LoRA (Low-Rank Adaptation)** para reentrenar los modelos en GPUs T4 de Google Colab, reduciendo drÃ¡sticamente el consumo de VRAM.
*   **HiperparÃ¡metros:** Ajustados mediante OptimizaciÃ³n Bayesiana con **Optuna**.

### ğŸ“Š Resultados
Ambos modelos alcanzaron una **Exactitud (Accuracy) del 86.3%** en el conjunto de test, superando ampliamente a los *baselines* zero-shot (69-75%).

| MÃ©trica | LLaMA-1B (Tuned) | Falcon-7B (Tuned) |
| :--- | :--- | :--- |
| **Accuracy** | **86.3%** | **86.3%** |
| **Precision** | 0.87 | 0.89 |
| **Recall** | 0.86 | 0.86 |
| **F1-Score** | 0.86 | 0.86 |

---

## ğŸ“‚ Estructura del Repositorio

El proyecto se divide en tres mÃ³dulos principales: **Entrenamiento** (Notebooks), **Backend** (LÃ³gica del Agente) y **Frontend** (Interfaz Web).

.
â”œâ”€â”€ chatwidget/                   # ğŸ¨ FRONTEND (React & TypeScript)
â”‚   â”œâ”€â”€ src/                      # Componentes del chat y lÃ³gica de UI
â”‚   â”œâ”€â”€ public/                   # Assets estÃ¡ticos
â”‚   â”œâ”€â”€ package.json              # Dependencias de Node.js
â”‚   â”œâ”€â”€ music_recommender.py      # ğŸ¤– BACKEND (Orquestador DialogueKit)
â”‚   â””â”€â”€ chatwidget.md             # DocumentaciÃ³n especÃ­fica del widget
â”‚
â”œâ”€â”€ colab_notebooks/              # ğŸ““ ENTRENAMIENTO E INFERENCIA
â”‚   â”œâ”€â”€ data/                     # Dataset (train.csv, test.csv, eval.csv)
â”‚   â”œâ”€â”€ ft_falcon_model/          # Checkpoints y logs de Falcon
â”‚   â”œâ”€â”€ ft_llama_model/           # Checkpoints y logs de LLaMA
â”‚   â”œâ”€â”€ apiFalcon.ipynb           # ğŸš€ Script de despliegue API (Falcon)
â”‚   â”œâ”€â”€ apiLlama.ipynb            # ğŸš€ Script de despliegue API (LLaMA)
â”‚   â”œâ”€â”€ Finetuning_Falcon.ipynb   # Entrenamiento LoRA + Optuna
â”‚   â”œâ”€â”€ Finetuning_LLaMa.ipynb    # Entrenamiento LoRA + Optuna
â”‚   â”œâ”€â”€ MergeModels.ipynb         # FusiÃ³n de pesos (Base + LoRA)
â”‚   â””â”€â”€ EvalLossPlot.ipynb        # GrÃ¡ficas de pÃ©rdidas (TensorBoard)
â”‚
â””â”€â”€ docs/                         # ğŸ“š DOCUMENTACIÃ“N
    â”œâ”€â”€ memoria.pdf               # Memoria completa del TFG
    â”œâ”€â”€ PresentacionTFG.pdf       # Diapositivas de defensa
    â””â”€â”€ PruebaChat.mp4            # Video demostrativo

---

## ğŸš€ GuÃ­a de InstalaciÃ³n y Despliegue

Debido a que los modelos LLM requieren GPU, el sistema utiliza una **arquitectura hÃ­brida**: el modelo corre en la nube (Colab) y la aplicaciÃ³n en local.

### Paso 1: Desplegar la API de Inferencia (Nube)
1.  Abre `colab_notebooks/apiLlama.ipynb` (o Falcon) en Google Colab.
2.  AsegÃºrate de seleccionar un entorno de ejecuciÃ³n con **GPU (T4)**.
3.  Ejecuta todas las celdas. Esto instalarÃ¡ las librerÃ­as necesarias y levantarÃ¡ un servidor Flask con **Localtunnel**.
4.  Copia la URL pÃºblica generada al final (ej: `https://dark-pugs-sing.loca.lt`).

### Paso 2: Configurar el Backend (Local)
1.  Navega a la carpeta `chatwidget/`.
2.  Instala las dependencias de Python:
    ```bash
    pip install -r requirements.txt
    ```
    *(Nota: AsegÃºrate de tener las librerÃ­as de `DialogueKit`, `Flask` y `sqlite3` instaladas).*
3.  Abre `music_recommender.py` y actualiza la variable `API_URL` con el enlace obtenido en el Paso 1.
4.  Inicia el agente conversacional:
    ```bash
    python music_recommender.py
    ```

### Paso 3: Iniciar el Frontend (Local)
1.  En una nueva terminal, navega a la carpeta `chatwidget/`.
2.  Instala las dependencias de Node.js:
    ```bash
    npm install
    ```
3.  Lanza el servidor de desarrollo:
    ```bash
    npm start
    ```
4.  Abre tu navegador en `http://localhost:3000`. Â¡El asistente estÃ¡ listo! ğŸ§

---

## ğŸ› ï¸ Stack TecnolÃ³gico

*   **Lenguajes:** Python 3.10+, TypeScript.
*   **Deep Learning:** PyTorch, Transformers (Hugging Face), PEFT (LoRA).
*   **OptimizaciÃ³n:** Optuna (BÃºsqueda Bayesiana de HiperparÃ¡metros).
*   **Backend:** Flask, DialogueKit (IAI Group).
*   **Frontend:** React, WebSockets.
*   **Base de Datos:** SQLite (CatÃ¡logo de 1M+ de canciones).
*   **Infraestructura:** Google Colab (GPU T4), Localtunnel.

---

## ğŸ“ Referencias
Este trabajo se fundamenta en la investigaciÃ³n de modelos generativos y su aplicaciÃ³n en PLN. Para mÃ¡s detalles tÃ©cnicos, consultar la carpeta `/docs`.

*   *Maestre GÃ³mez, A. (2025). Desarrollo de un Sistema Asistente de MÃºsica Basado en Aprendizaje AutomÃ¡tico.* Universidad de Huelva.
*  ([https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct))
*  ([https://huggingface.co/tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b))
